---
title: "Project 10 Structured sparsity in genetics"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author: "Team K - Minghang Li"
mainfont: "KpRoman"
monofont: "Fira Code Regular Nerd Font Complete"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:  [ "amsthm", "amssymb", "cancel", "unicode-math" ]
---

```{r, include=FALSE}
library(caret)
library(glmnet)
library(pROC)
library(foreach)
library(doParallel)
library(dplyr)
library(doRNG)
```

# Problem 28: Uniqueness of predictions from the lasso

\begin{proof}
To prove that $\mathbf{X} \hat{\beta}_1 = \mathbf{X} \hat{\beta}_2$, it's equivalent to prove $\hat{\beta}_1 = \hat{\beta}_2$. We'll prove by contradiction.

Let the solution set be denoted as $S$, we have $\hat{\beta}_1, \hat{\beta}_2 \in S$.

Since $S$ is convex, we also have $\alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2 \in S$.

So that we have:

\begin{equation} \label{eq:1}
\frac{1}{2}
\left\lVert\ 
  \mathbf{y} - \mathbf{X} \left(
    \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
  \right)
\right\rVert
+
\lambda \left\lVert
\alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right\rVert
=
c^*
\end{equation}

Define the loss function as $f: S \mapsto \lVert \mathbf{y} - \mathbf{X} \beta \rVert_2^2$. We know that the lineare loss function is stirctly convex, so that we have:

\begin{equation} \label{eq:2}
f\left(
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right) <
\alpha f(\hat{\beta}_1) +
(1 - \alpha)f(\hat{\beta}_2)
\end{equation}

Similarly, since $l_1$-nrom is convex, we have:

\begin{equation} \label{eq:3}
\left\lVert
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right\rVert_1
\leq
\alpha\lVert \hat{\beta}_1 \rVert
+ (1 - \alpha) \lVert \hat{\beta}_2 \rVert
\end{equation}

Combining Equation \ref{eq:2} and Equation \ref{eq:3}, we can start the following derivation:

$$
\begin{aligned}
&\phantom{= }\frac{1}{2}
f\left(
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right) + \lambda \left\lVert
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right\rVert \\
&<
\alpha\left(
  f(\hat{\beta}_1) + \lambda \lVert\hat{\beta}_1\rVert
\right) +
(1 - \alpha) \left(
  f(\hat{\beta}_2) + \lambda \lVert\hat{\beta}_2\rVert)
\right) \\
&= \alpha c^* + (1-\alpha) c^* \\
&= c^*
\end{aligned}
$$
Just a re-write:

\begin{equation} \label{eq:4}
\frac{1}{2}
f\left(
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right) + \lambda \left\lVert
  \alpha\hat{\beta}_1 + (1 - \alpha)\hat{\beta}_2
\right\rVert
< c^*
\end{equation}

Equation \ref{eq:4} contradicts with Equation \ref{eq:1}, which suggests that our assumption $\hat{\beta}_1 \neq \hat{\beta}_2$ is wrong.

\end{proof}

# Problem 29: Ridge regression solution

Given $n \times p$ matrix $\mathbf{X}$, response $n$-vector $\mathbf{y}$, and the parameter $p$-vector $\beta$, consider augmenting $\mathbf{X}$ with rows corresponding to $\sqrt{\lambda}$ times a $p \times p$ identity matrix $I$:

$$
\mathbf{X}_* = \begin{pmatrix}
  \mathbf{X} \\
  \sqrt{\lambda} \mathbf{I}
\end{pmatrix}
$$

and $\mathbf{y}$ is similarly augmented with $p$ zeros at its end into $\mathbf{y}_*$.

Now, the least square objective function on the modified dataset turns out to be:

\begin{equation} \label{eq:5}
\begin{aligned}
&\phantom{=} (\mathbf{y}_* - \mathbf{X}_* \beta)^T
(\mathbf{y}_* - \mathbf{X}_* \beta) \\
&= 
  \mathbf{y}_*^T \mathbf{y}_* 
- 2\mathbf{y}_*^T\mathbf{X}_*\beta 
+ \beta^T\mathbf{X}_*^T\mathbf{X}_*\beta
\end{aligned}
\end{equation}


The solution to the linear regression on the modified dataset hence follows:

\begin{equation} \label{eq:6}
\hat{\beta} = 
  (\mathbf{X}_*^T\mathbf{X}_*)^{-1}
  \mathbf{X}_*^T\mathbf{y}
\end{equation}

Since we are just padding $\mathbf{y}$ with zeros, the augmented part of $\mathbf{X}_*$ will not have any effect when it is multiplied by $\mathbf{y}_*^T$:

\begin{equation} \label{eq:7}
\mathbf{y}_*^T \mathbf{X}_* = \mathbf{y}^T\mathbf{X}
\end{equation}

And finally with augmented dataset $\mathbf{X}_*$ we have:

\begin{equation} \label{eq:8}
\begin{aligned}
&\phantom{=}\mathbf{X}_*^T\mathbf{X}_* \\
&= \begin{pmatrix}
  \mathbf{X}^T & \sqrt{\lambda}\mathbf{I}
\end{pmatrix} \begin{pmatrix}
  \mathbf{X} \\ \sqrt{\lambda}\mathbf{I}
\end{pmatrix} \\
&= \mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}
\end{aligned}
\end{equation}

Plug Equation \ref{eq:7} and Equation \ref{eq:8} back into Equation \ref{eq:6}, we delightfully sees that performing linear regression on the modified dataset gives:

\begin{equation}
\hat{\beta} 
= (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1}
  \mathbf{X}^T \mathbf{y}
\end{equation}

which is just the ridge regression solution.

\pagebreak

# Problem 30: Variable selection under various norms

## Load data and split into training and test set

```{r}
# load data
set.seed(42)
load("yeastStorey.rda")
```

```{r}
sample <- createDataPartition(data$Marker, p = 0.7, list = FALSE)

train <- data[sample, ]
test  <- data[-sample, ]
```

```{r}
X_train <- as.matrix(train[, -1])
y_train <- train[,  1]

X_test  <- as.matrix(test[, -1])
y_test  <- test[,  1]
```


## Do 10-fold cross validation

```{r}
alphas <- seq(0, 1, 0.1)
```

```{r}
foldid <- sample(1:10, size = length(sample), replace = TRUE)

num_cores <- detectCores()
registerDoParallel(num_cores)
start <- Sys.time()
res <- foreach (alpha = alphas, .combine = c, .options.RNG = 42) %dorng% {
  cv_res <- cv.glmnet(X_train, y_train, alpha = alpha,
                      family = "binomial", foldid = foldid,
                      type.measure = "mse")
  list(cv_res)
}
end <- Sys.time()
end - start
stopImplicitCluster()
```
```{r}
df <- do.call(rbind, 
              lapply(1:length(alphas), 
                     function(x) {
                      cbind.data.frame(alphas[x], 
                                       res[[x]]$lambda, 
                                       res[[x]]$cvm)
                     })
              )
colnames(df) <- c("alpha", "lambda", "cvm")
```

```{r}
param <- filter(df, cvm == min(cvm))
param
```

For the optimal $\alpha$ (here $\alpha = 1$)...

```{r}
opt_idx <- which(alphas == param$alpha)
res.opt_alpha <- res[[opt_idx]]
```

Plot the mean cross-validated error as a function of $\log \lambda$.

```{r}
plot(res.opt_alpha)
```
Plot the trace curve of coefficients as a function of $\log \lambda$.

```{r}
plot(res.opt_alpha$glmnet.fit, "lambda", label=TRUE)
```

## Fit the final model and predict the response

```{r}
model <- glmnet(X_train, y_train, 
                alpha = param$alpha,
                family = "binomial",
                lambda = param$lambda)
```

```{r}
prob <- predict(model, newx = X_test, type = "response")
y_pred <- ifelse(prob > 0.5, 1, 0)
```

Report the varaibles selected.

```{r}
selected <- coef(res.opt_alpha, param$lambda)
selected@Dimnames[[1]][selected@i]
```

Plot ROC...

```{r}
test_roc <- roc(y_test, as.vector(y_pred))
plot(test_roc)
```

... and report the corresponding AUC.

```{r}
test_roc$auc
```
