---
title: "Project 8 Nested Effect Models"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author: "Team C - Minghang Li, Xiaocheng Yang, Xinyi Chen"
mainfont: "KpRoman"
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
output:
  pdf_document:
    latex_engine: xelatex
    pandoc_args: "--pdf-engine-opt=-shell-escape"
    extra_dependencies: ["amsmath", "amsthm", "amssymb", "cancel", "unicode-math" ]
---

```{r dependencies, include=FALSE}
library(mnem)
library(doParallel)
library(foreach)
library(ggplot2)
```

# Problem 20: Classical NEMs

## Subproblem 1

### Construct transitive closure and define $\Phi$

Define a function to make any $\Phi$ matrix transitive closed by powering it up
 until convergence.

Construct $\Phi$ for Model (a).

```{r init_phi_a}
phi_a <- array(dim = c(5, 5),
               dimnames = list(c("S1", "S2", "S3", "S4", "S5"),
                               c("S1", "S2", "S3", "S4", "S5")))
#                  S1, S2, S3, S4, S5
phi_a["S1",] <- c(1,  0,  1,  1,  0)
phi_a["S2",] <- c(0,  1,  0,  0,  1)
phi_a["S3",] <- c(0,  0,  1,  1,  1)
phi_a["S4",] <- c(0,  0,  0,  1,  1)
phi_a["S5",] <- c(0,  0,  0,  0,  1)

phi_a <- transitive.closure(phi_a)
phi_a
```

Construct $\Phi$ for Model (b).

```{r init_phi_b}
phi_b <- array(dim = c(5, 5),
               dimnames = list(c("S1", "S2", "S3", "S4", "S5"),
                               c("S1", "S2", "S3", "S4", "S5")))
#                  S1, S2, S3, S4, S5
phi_b["S1",] <- c(1,  0,  0,  1,  0)
phi_b["S2",] <- c(0,  1,  0,  0,  1)
phi_b["S3",] <- c(1,  0,  1,  1,  1)
phi_b["S4",] <- c(0,  0,  0,  1,  1)
phi_b["S5",] <- c(0,  0,  0,  0,  1)

phi_b <- transitive.closure(phi_b)
phi_b
```

### Define $\Theta$

Define $\Theta$ for Model (a).

```{r init_theta_a}
theta_a <- array(dim = c(5, 6),
                 dimnames = list(
                   c("S1", "S2", "S3", "S4", "S5"),
                   c("E1", "E2", "E3", "E4", "E5", "E6")
                 ))
#                    E1, E2, E3, E4, E5, E6
theta_a["S1",] <- c(0,  0,  0,  0,  0,  0)
theta_a["S2",] <- c(0,  0,  0,  1,  0,  1)
theta_a["S3",] <- c(1,  1,  0,  0,  0,  0)
theta_a["S4",] <- c(0,  0,  1,  0,  0,  0)
theta_a["S5",] <- c(0,  0,  0,  0,  1,  0)
theta_a
```

Define $\Theta$ for Model (b).

```{r init_theta_b}
theta_b <- array(dim = c(5, 6),
                 dimnames = list(
                   c("S1", "S2", "S3", "S4", "S5"),
                   c("E1", "E2", "E3", "E4", "E5", "E6")
                 ))
#                    E1, E2, E3, E4, E5, E6
theta_b["S1",] <- c(1,  1,  0,  0,  0,  0)
theta_b["S2",] <- c(0,  0,  0,  1,  0,  1)
theta_b["S3",] <- c(0,  0,  0,  0,  0,  0)
theta_b["S4",] <- c(0,  0,  1,  0,  0,  0)
theta_b["S5",] <- c(0,  0,  0,  0,  1,  0)
theta_b
```

### Determine the corresponding expected effect patterns ($F$)

```{r expected_effect_pattern_A}
F_a <- phi_a %*% theta_a
F_a
```

```{r expected_effect_pattern_B}
F_b <- phi_b %*% theta_b
F_b
```

## Subproblem 2

If we assume no noise (no false positives and false negatives)... then the $D$
 matrix is simply the $F$ matrix transpose.

```{r D_a}
D_a <- t(F_a)
D_a
```

```{r D_b}
D_b <- t(F_b)
D_b
```

Given the discrete data $D_a$ and $D_b$ (sorry for the different notation from
 the exercise pdf) it's not possible to tell apart the two models because they
  are identical.

```{r assume_equal}
all.equal(D_a, D_b)
```

## Subproblem 3

Calculate the marginal log-likelihood ratio (network score) given the data by
 setting the false positive rate to be 5\% and the false negative rate to be 1\%.

```{r network_score_a}
network_score_a <- scoreAdj(
  D_a,
  adj = phi_a,
  method = "disc",
  fpfn = c(0.05, 0.01),
  logtype = exp(1)
)$score
network_score_a
```

```{r network_score_b}
network_score_b <- scoreAdj(
  D_b,
  adj = phi_b,
  method = "disc",
  fpfn = c(0.05, 0.01),
  logtype = exp(1)
)$score
network_score_b
```

# Problem 21: Hidden Markov NEMs

## Subproblem 1

Compute the transition probabilities from $G_t = u$ to $G_{t+1} \in \{ v_1, v_2 \}$
 for different smoothness parameter $\lambda \in \{0.1, \ldots, 0.9\}$.

By definition, the probability of transition from network $u$ to network $v$ is
 calculated by:

$$
\begin{aligned}
T_{uv} &= P(\Phi_{t+1} = v | \Phi_{t} = u) \\
       &= \frac{1}{C_u} (1 - \lambda)^{s_{uv}} \cdot \lambda
\end{aligned}
$$

The distance $s_{uv}$ is defined as

$$
s_{uv} = || u - v ||_1 := \sum_i \sum_{i'} |u_{ii'} - v_{ii'}|
$$

The normalizing constant $C_u$ is defined as

$$
C_u = \sum_w (1- \lambda)^{s_{uw}} \cdot \lambda
$$
where $w$ is all possible networks given the S genes at hand.

So the basic implementation idea would be:

1. Represent $u$, $v_1$ and $v_2$ using adjacency matrix
2. Compute $s_{uv_1}$ and $s_{uv_2}$ by "diff"ing the pairs of matrices respectively
3. Generate all the networks $w$ using `mnem`, compute all the $s_{uw}$.
4. Compute transition probability for each $\lambda$

Implementation in R is in the following code blocks.

```{r s_gene_names}
S_genes <- c("S1", "S2", "S3", "S4")
```

```{r init_u}
# Initialization of u
u <- array(dim = c(4, 4),
           dimnames = list(S_genes, S_genes))
#              S1, S2, S3, S4
u["S1",] <- c(1,  1,  1,  0)
u["S2",] <- c(0,  1,  1,  1)
u["S3",] <- c(0,  0,  1,  1)
u["S4",] <- c(0,  0,  0,  1)
u <- transitive.closure(u)
u
```

```{r init_v1}
# Initialization of v1
v1 <- array(dim = c(4, 4),
            dimnames = list(S_genes, S_genes))
#               S1, S2, S3, S4
v1["S1",] <- c(1,  1,  1,  0)
v1["S2",] <- c(0,  1,  1,  1)
v1["S3",] <- c(0,  0,  1,  0)
v1["S4",] <- c(0,  0,  0,  1)
v1 <- transitive.closure(v1)
v1
```


```{r init_v2}
# Initialization of v2
v2 <- array(dim = c(4, 4),
            dimnames = list(S_genes, S_genes))
#               S1, S2, S3, S4
v2["S1",] <- c(1,  0,  0,  0)
v2["S2",] <- c(1,  1,  1,  0)
v2["S3",] <- c(1,  0,  1,  0)
v2["S4",] <- c(1,  0,  0,  1)
v2 <- transitive.closure(v2)
v2
```

```{r compute_s_uvs}
# compute s_uv1 and s_uv2
s_uv1 <- sum(u != v1)
s_uv2 <- sum(u != v2)
```

```{r generate_all_networks}
# generate all the possible networks
all_networks <- mnem:::enumerate.models(S_genes, trans.close = TRUE)
```

```{r compute_s_uw, results='hide'}
# compute s_uw for all networks
num_cores <- detectCores()
registerDoParallel(num_cores)
start <- Sys.time()
s_uw <-
  foreach (
    i = 1:length(all_networks),
    .combine = c,
    .packages = c("foreach")
  ) %dopar% {
    sum(u != all_networks[[i]])
  }
end <- Sys.time()
end - start
stopImplicitCluster()
```

```{r compute_C}
compute_C <- function(lambda, s_uw) {
  res <-
    foreach (
      i = 1:length(s_uw),
      .combine = c,
      .packages = c("foreach")
    ) %dopar% {
      (1 - lambda) ^ s_uw[i] * lambda
    }
  return(sum(res))
}
```

```{r compute_trans_prob}
# Compute transitive probability
registerDoParallel(num_cores)
start <- Sys.time()
trans_prob <-
  foreach (
    lambda = seq(0.1, 0.9, by = 0.1),
    .combine = rbind,
    .packages = c("foreach")
  ) %dopar% {
    C_u <- compute_C(lambda = lambda, s_uw = s_uw)
    res1 <- (1 - lambda) ^ s_uv1 * lambda / C_u
    res2 <- (1 - lambda) ^ s_uv2 * lambda / C_u
    return(c(res1, res2))
  }
end <- Sys.time()
end - start
stopImplicitCluster()
colnames(trans_prob) <- c("v1", "v2")
rownames(trans_prob) <- sprintf("%.1f", seq(0.1, 0.9, 0.1))
trans_prob
```

## Subproblem 2

Plot the transition probabilites as a function of $\lambda$ for $v_1$ and $v_2$.

```{r make_df}
df <-
  data.frame(lambda = row.names(trans_prob),
             trans_prob,
             row.names = NULL)
df$lambda <- as.numeric(df$lambda)
```

```{r plot_trans_prob}
ggplot(data = df, aes(x = lambda, group = 1)) +
  geom_line(aes(y = v1, color = "v1"), linetype = "dashed") +
  geom_point(aes(y = v1, color = "v1"), size = 2) +
  geom_line(aes(y = v2, color = "v2"), linetype = "dashed") +
  geom_point(aes(y = v2, color = "v2"), size = 2) +
  xlab("lambda") +
  ylab("Transition probability") +
  ggtitle("Transition probability from u to v1 and v2\nas a Function of lambda")
```

We can see that the transition probabilities of $v_1$ and $v_2$ converge when
 $\lambda$ is small and differ greatly when $\lambda$ is large. Dissimilar
 networks get penalized and result in lower transition probability as $\lambda$
 increase. For similar networks like $v_1$, the probability to transit into them
 increases as we increase $\lambda$.

# Problem 22: Mixture NEMs

## Subproblem 1

Determine cellular perturbation map $\rho$ where $\rho_{ic} = 1$ if cell $c$
 is perturbed by a knowdown of S-gene $i$.

```{r name_genes_and_cells}
S_genes <- c("S1", "S2")
E_genes <- c("E1", "E2")
cells   <- c("C1", "C2", "C3", "C4")
```

```{r init_rho}
rho = array(dim = c(2, 4),
            dimnames = list(S_genes, cells))
#               C1, C2, C3, C4
rho["S1",] <- c(1,  0,  1,  0)
rho["S2",] <- c(0,  1,  1,  1)
rho
```

## Subproblem 2

Assume $\{C_1, C_2\}$ are generated from $F_1$ and $\{C_3, C_4\}$ are generated
 from $F_2$, compute the *noiseless* log odds matrix $R$, where $R_{jc} > 0$
 means that the perturbation on cell $c$ has an effect on E-gene $j$.

### (a) Compute expected effect pattern $(\rho^T\phi_k\theta_k)^T$

Of course need to define $\Phi$ and $\Theta$ again for $F_1$ and $F_2$.

```{r define_phi_theta_1}
phi_1 <- array(dim = c(2, 2), dimnames = list(S_genes, S_genes))
#                 S1, S2
phi_1["S1",] <- c(1,  1)
phi_1["S2",] <- c(0,  1)

theta_1 <- array(dim = c(2, 2), dimnames = list(S_genes, E_genes))
#                   E1, E2
theta_1["S1",] <- c(1,  0)
theta_1["S2",] <- c(0,  1)
```

```{r define_phi_theta_2}
phi_2 <- array(dim = c(2, 2), dimnames = list(S_genes, S_genes))
#                  S1, S2
phi_2["S1", ] <- c( 1,  0)
phi_2["S2", ] <- c( 1,  1)

theta_2 <- array(dim = c(2, 2), dimnames = list(S_genes, E_genes))
#                    E1, E2
theta_2["S1", ] <- c( 0,  1)
theta_2["S2", ] <- c( 1,  0)
```

```{r compute_EEP_1}
EEP_1 <- t(t(rho) %*% phi_1 %*% theta_1)
EEP_1[EEP_1 > 1] <- 1
EEP_1
```

```{r compute_EEP_2}
EEP_2 <- t(t(rho) %*% phi_2 %*% theta_2)
EEP_2[EEP_2 > 1] <- 1
EEP_2
```

### (b) Extract the corresponding colum from the expected effect patterns and put it into $R$

```{r compute_R}
# C1, C2 from F1; C3, C4 from F2
R <- cbind(EEP_1[, 1:2], EEP_2[, 3:4])
R[R == 0] <- -1
R
```

## Subproblem 3

Calculate the responsibilities $\Gamma$ given mixture weights $\pi = (0.44, 0.56)$.
 Then update the mixture weights. (Well, EM again!)

```{r init_pi}
pi <- c(0.44, 0.56)
```

```{r compute_gamma}
# log likelihood
L1 <- t(EEP_1) %*% R
L2 <- t(EEP_2) %*% R

gamma <- rbind(diag(pi[1] * exp(L1) / (pi[1] * exp(L1) + pi[2] * exp(L2))),
               diag(pi[2] * exp(L2) / (pi[1] * exp(L1) + pi[2] * exp(L2))))
gamma
```

```{r update_pi}
pi[1] <- mean(gamma[1,])
pi[2] <- mean(gamma[2,])
```

```{r print_pi}
pi
```
