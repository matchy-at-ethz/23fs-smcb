---
title: "Project 2 EM Algorithm"
date: "March 24, 2023"
author: Team K
output: pdf_document
---

```{r, include=FALSE}
source("code/viterbi.r", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
```

# Problem 6: Hidden Markov Models

According to slides 18, the HMM is parameterized by:

$$
\begin{aligned}
&\text{Initial state probabilities: }& I_k = P(Z_1 = k) \\
&\text{Transition probabilities: } & T_{kl} = P(Z_n=l|Z_{n-1}=k) \\
&\text{Emission probabilities: } & E_{kx} = P(X_n = x | Z_n = k)
\end{aligned}
$$

## (a)

$I_k$ should sum up to $1$ $\longrightarrow$ degree of freedom = $K-1$

In the $K \times K$ matrix $T$, each row needs to sum up to $1$
$\longrightarrow$ degree of freedom = $K-1$ for each row

Similarly, in the $K \times M$ matrix $X$, each row needs to sum up to $1$
$\longrightarrow$ degree of freedom = $M-1$ for each row

Hence, the maximum number of free parameters is
$(K-1) + K \times (K-1) + K\times (M-1)$

## (b)

Computing the stationary distribution is equivalent to solving $\pi^t = \pi^t T$.

Let $\pi^t = \begin{bmatrix}\pi_1 & \pi_2\end{bmatrix})$, we need to solve:

$$
\begin{aligned}
\begin{bmatrix}\pi_1 & \pi_2\end{bmatrix}
&= \begin{bmatrix}
  \pi_1& \pi_2
\end{bmatrix}\begin{bmatrix}
  0.2 & 0.8 \\
  0.6 & 0.4
\end{bmatrix} \\
&= \begin{bmatrix}
  0.2\pi_1 + 0.6 \pi_2 &
  0.8\pi_1 + 0.4 \pi_2
\end{bmatrix}
\end{aligned}
$$

Given that $\pi_1 = 1 - \pi_2$e can solve the equations:

$$
\left\{\begin{aligned}
  &\pi_1 = 0.2\pi_1 + 0.6\pi_2\\
  &\pi_2 = 0.8\pi_1 + 0.4\pi_2\\
  &\pi_1 = 1 - \pi_2
\end{aligned}\right.
\Rightarrow
\left\{\begin{aligned}
  &\pi_1 = \frac{3}{7}\\
  &\pi_2 = \frac{4}{7}
\end{aligned}\right.
$$

Alternatively we can solve by solving eigens (which is porbably the more proper
way to do it because solving the equations above by hand for high dimensional
transition matrices is not a very smart idea). Since $\pi^t = \pi^tT$, we can
re-write it into $\pi = T^t \pi$, which means $\pi$ is just the corresponding
eigenvector of eigenvalue $\lambda = 1$.

Note that the eigenvector should be normalized to satisfy the constraint that
$\pi_1 + \pi_2 = 1$.

```{r}
# initialize the transition matrix
T <- matrix(c(0.2, 0.8, 0.6, 0.4), 2, 2, byrow=TRUE)

# solve for the eigenvalues and eigenvectors for transpose of T
eigens <- eigen(t(T))

# get the index of lambda = 1
index <- which(eigens$values == 1)[1]
ev <- eigens$vectors[, index]

# normalize so that the probs sum to 1
pi <- ev / sum(ev)
pi
```

```{r}
# check that the two computation confirms
abs(pi[1] - 3/7) < 1e-5
abs(pi[2] - 4/7) < 1e-5
```

# Problem 7: Predictinig protein secondary structure using HMMs

## (a)

Read `proteins train.tsv`, `proteins test.tsv` and `proteins new.tsv` into the
memory and store each in a `data.frame`

```{r}
train <- read.csv("data/proteins_train.tsv", sep="\t", header=F)
test  <- read.csv("data/proteins_test.tsv",  sep="\t", header=F)
new   <- read.csv("data/proteins_new.tsv",   sep="\t", header=F)
```

```{r}
header <- c("identifier",
             "sequence",
             "structure")
colnames(train) <- header
colnames(test)  <- header
colnames(new)   <- header[1:2]
```

## (b)

Estimate the vector of initial state probabilities $I$, the matrix of transition
probabilities $T$ and the matrix for emission probabilities $E$ by maximum
likelihood.

```{r}
forward_prob <- function() {
  # placeholder
}
```

```{r}
backward_prob <0- function() {
  # placeholder
}
```

```{r}
baum_welch <- function(Tr, E,
                       state_label, symbol_label, threshold=1e-5, max_iter=1000) {

  # get the number of states and symbols
  K <- length(state_label)
  M <- length(symbol_label)

  # set Tr and E to all zeros
  Tr <- matrix(0, K, K)
  E  <- matrix(0, K, M)

  ll_hid <- -Inf
  ll_obs <- -Inf
  diff <- Inf
  iter <- 0
  while (diff >= threshold && iter < max_iter) {

    ll_obs_old <- ll_obs
    # E-step
    # compute the forward and backward probabilities


    # M-step
    # compute the new transition and emission matrices

    # update the log-likelihood
    log_likelihood <- log_likelihood_new

    # update the iteration
    iter <- iter + 1
  }
}
``


## (c)

Estimate the stationary distribution $\pi$ of the Markov chain by solving the
eigenvalue problem and by using a brute-force approach.

### Eigenvalue method

### Brute-force

## (d)

Having estimated the parameters, i.e., the emission and transition matrices $E$,
$T$ and the vector of initial state probabilities $I$, you can predict the
latent state sequence $Z$ of a proteinâ€™s amino acid sequence $X$ using the
Viterbi algorithm. Use the Viterbi algorithm provided in `viterbi.r` (carefully
read the parameter description!) and iterate over each `data.frame` of
`proteinstest.tsv` and `proteins new.tsv` row by row and use the amino acid
sequence to predict its secondary structure, which you add to the `data.frame`
as a new column. Save the extended `data.frame` of `proteins new.tsv` including the
predicted secondary structure as a tsv file and hand it in together with your
pdf.

## (e)

Estimate confidence intervals for each parameter in $I$, $E$ and $T$ with
bootstrapping. In a single bootstrap run $i$ estimate the probabilities for
$I_i$, $E_i$ and $T_i$ the same as before, but not on the original data set
`proteins train.tsv`, but on the resampled data set. i.e., sample with
replacement as many rows from `proteins train.tsv` as the original data set has.
Run a thousand bootstraps and compute the empirical $95\%$ confidence intervals
for each single parameter in $\{I_i\}_i$, $\{E_i\}_i$ and $\{T_i\}_i$.

## (f)

Use the following measure to compute the accuracy of the predicted secondary
structure $P = (p_i)$ for the `data.frame` of `proteins test.tsv` given the real
secondary structure $S = (s_i)$:

$$
a(P, S) =
\frac{1}{L} \sum_{i} \left\{\begin{aligned}
&1 & \text{if } p_i = s_i\\
&0 & \text{if } p_i \neq s_i
\end{aligned}\right.
$$

with sequence length $L$. Compute the accuracy for every protein in your
`data.frame` and store the accuracies in a vector. What is the accuracy of the
Viterbi algorithm over all sequences (i.e. call `summary` on the vector of
accuracies)?

## (g)

Instead of using the Viterbi algorithm, now randomly guess secondary structures
for all sequences. Compare the global accuracies of the Viterbi and the random
approach and plot all accuracy distributions using boxplots.
