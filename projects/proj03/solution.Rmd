---
title: "Project 2 EM Algorithm"
date: "March 24, 2023"
author: Team K
output: pdf_document
---

```{r, include=FALSE}
source("code/viterbi.r", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
```

```{r, include=FALSE}
library(tidyverse)
```

# Problem 6: Hidden Markov Models

According to slides 18, the HMM is parameterized by:

$$
\begin{aligned}
&\text{Initial state probabilities: }& I_k = P(Z_1 = k) \\
&\text{Transition probabilities: } & T_{kl} = P(Z_n=l|Z_{n-1}=k) \\
&\text{Emission probabilities: } & E_{kx} = P(X_n = x | Z_n = k)
\end{aligned}
$$

## (a)

$I_k$ should sum up to $1$ $\longrightarrow$ degree of freedom = $K-1$

In the $K \times K$ matrix $T$, each row needs to sum up to $1$
$\longrightarrow$ degree of freedom = $K-1$ for each row

Similarly, in the $K \times M$ matrix $X$, each row needs to sum up to $1$
$\longrightarrow$ degree of freedom = $M-1$ for each row

Hence, the maximum number of free parameters is
$(K-1) + K \times (K-1) + K\times (M-1)$

## (b)

Computing the stationary distribution is equivalent to solving $\pi^t = \pi^t T$.

Let $\pi^t = \begin{bmatrix}\pi_1 & \pi_2\end{bmatrix})$, we need to solve:

$$
\begin{aligned}
\begin{bmatrix}\pi_1 & \pi_2\end{bmatrix}
&= \begin{bmatrix}
  \pi_1& \pi_2
\end{bmatrix}\begin{bmatrix}
  0.2 & 0.8 \\
  0.6 & 0.4
\end{bmatrix} \\
&= \begin{bmatrix}
  0.2\pi_1 + 0.6 \pi_2 &
  0.8\pi_1 + 0.4 \pi_2
\end{bmatrix}
\end{aligned}
$$

Given that $\pi_1 = 1 - \pi_2$e can solve the equations:

$$
\left\{\begin{aligned}
  &\pi_1 = 0.2\pi_1 + 0.6\pi_2\\
  &\pi_2 = 0.8\pi_1 + 0.4\pi_2\\
  &\pi_1 = 1 - \pi_2
\end{aligned}\right.
\Rightarrow
\left\{\begin{aligned}
  &\pi_1 = \frac{3}{7}\\
  &\pi_2 = \frac{4}{7}
\end{aligned}\right.
$$

Alternatively we can solve by solving eigens (which is porbably the more proper
way to do it because solving the equations above by hand for high dimensional
transition matrices is not a very smart idea). Since $\pi^t = \pi^tT$, we can
re-write it into $\pi = T^t \pi$, which means $\pi$ is just the corresponding
eigenvector of eigenvalue $\lambda = 1$.

Note that the eigenvector should be normalized to satisfy the constraint that
$\pi_1 + \pi_2 = 1$.

```{r}
# initialize the transition matrix
Tr <- matrix(c(0.2, 0.8, 0.6, 0.4), 2, 2, byrow=TRUE)

# solve for the eigenvalues and eigenvectors for transpose of T
eigens <- eigen(t(Tr))

# get the index of lambda = 1
index <- which(eigens$values %>% near(1.0))[1]
ev <- eigens$vectors[, index]

# normalize so that the probs sum to 1
pi <- ev / sum(ev)
pi
```

```{r}
# check that the two computation confirms
abs(pi[1] - 3/7) < 1e-5
abs(pi[2] - 4/7) < 1e-5
```

# Problem 7: Predictinig protein secondary structure using HMMs

## (a)

> Read `proteins train.tsv`, `proteins test.tsv` and `proteins new.tsv` into the
> memory and store each in a `data.frame`

```{r}
train <- read.csv("data/proteins_train.tsv", sep = "\t", header = FALSE)
test  <- read.csv("data/proteins_test.tsv",  sep = "\t", header = FALSE)
new   <- read.csv("data/proteins_new.tsv",   sep = "\t", header = FALSE)
```

```{r}
header <- c("identifier",
             "sequence",
             "structure")
colnames(train) <- header
colnames(test)  <- header
colnames(new)   <- header[1:2]
```

```{r}
head(train)
```

## (b)

> Estimate the vector of initial state probabilities $I$, the matrix of transition
> probabilities $T$ and the matrix for emission probabilities $E$ by maximum
> likelihood.

First get all the possible amino acid and secondary structure states.

```{r}
# get all the possible amino acid states
aa_states <- train$sequence %>%
  strsplit("") %>%
  unlist() %>%
  unique() %>%
  sort()
aa_states
```

```{r}
# get all the possible secondary structure states, and sort them
ss_states <- train$structure %>%
  strsplit("") %>%
  unlist() %>%
  unique() %>%
  sort()
ss_states
```

```{r}
MLE <- function(data, aa_states, ss_states) {
  k <- length(ss_states) # num of latent state
  m <- length(aa_states) # num of observed state

  # initialize the parameters
  I <- rep(0.0, k)
  names(I) <- ss_states

  E <- matrix(0.0, nrow=k, ncol=m)
  dimnames(E) <- list(ss_states, aa_states)

  Tr <- matrix(0.0, nrow=k, ncol=k)
  dimnames(Tr) <- list(ss_states, ss_states)

  N <- nrow(data) # num of data points

  # iterate over each row of the data
  # TODO: maybe rewrite the for loops
  for (i in 1:N) {
    seq <- data$sequence[i]
    struct <- data$structure[i]

    ss_1st <- struct %>% substr(1, 1)
    I[ss_1st] <- I[ss_1st] + 1.0

    for (j in 1:nchar(seq)) {
      aa <- seq %>% substr(j, j)
      ss <- struct %>% substr(j, j)

      E[ss, aa] <- E[ss, aa] + 1.0

      if (j < nchar(seq)) {
        ss_next <- struct %>% substr(j+1, j+1)

        Tr[ss, ss_next] <- Tr[ss, ss_next] + 1.0
      }
    }
  }

  # convert thee counts to log probs
  I <- I / sum(I)
  E <- E / rowSums(E)
  Tr <- Tr / rowSums(Tr)

  return(list(I=I, E=E, Tr=Tr))
}
```

```{r}
res <- MLE(train, aa_states, ss_states)
```

```{r}
res$I
```

```{r}
res$E
```

```{r}
res$Tr
```

## (c)

> Estimate the stationary distribution $\pi$ of the Markov chain by solving the
> eigenvalue problem and by using a brute-force approach.

### Eigenvalue method

```{r}
eigens <- eigen(t(res$Tr))

index <- which(eigens$values %>% near(1.0))[1]

ev <- eigens$vectors[, index]

pi_eigen <- ev / sum(ev)
pi_eigen
```

### Brute-force

```{r}
# compute Tr %*% Tr until there's no huge difference between the two
# matrices
Tr <- res$Tr
pi_brute <- rep(0.0, length(ss_states))
pi_brute[1] <- 1.0

while (TRUE) {
  Tr <- Tr %*% res$Tr
  pi_new <- Tr[1, ]

  if (max(abs(pi_new - pi_brute)) < 1e-5) {
    break
  }

  pi_brute <- pi_new
}
```

```{r}
pi_brute %>% near(pi_eigen)
```

## (d)

> Having estimated the parameters, i.e., the emission and transition matrices $E$,
> $T$ and the vector of initial state probabilities $I$, you can predict the
> latent state sequence $Z$ of a proteinâ€™s amino acid sequence $X$ using the
> Viterbi algorithm. Use the Viterbi algorithm provided in `viterbi.r` (carefully
> read the parameter description!) and iterate over each `data.frame` of
> `proteins_test.tsv` and `proteins_new.tsv` row by row and use the amino acid
> sequence to predict its secondary structure, which you add to the `data.frame`
> as a new column. Save the extended `data.frame` of `proteins_new.tsv` including the
> predicted secondary structure as a tsv file and hand it in together with your
> pdf.

```{r}
predict_test <- test[,2]
colnames(predict_test) <- c("AminoAcids")
predict_test <- viterbi(log(res$E), log(res$Tr), log(res$I), predict_test)
```

```{r}
predict_new <- new[,2]
colnames(predict_new) <- c("AminoAcids")
predict_new <- viterbi(log(res$E), log(res$Tr), log(res$I), predict_new)
```

```{r}
# save predict new to a tsv file
write.table(predict_new, file="proteins_struct_new.tsv", sep="\t", quote=FALSE, row.names=FALSE)
```

## (e)

> Estimate confidence intervals for each parameter in $I$, $E$ and $T$ with
> bootstrapping. In a single bootstrap run $i$ estimate the probabilities for
> $I_i$, $E_i$ and $T_i$ the same as before, but not on the original data set
> `proteins train.tsv`, but on the resampled data set. i.e., sample with
> replacement as many rows from `proteins train.tsv` as the original data set has.
> Run a thousand bootstraps and compute the empirical $95\%$ confidence intervals
> for each single parameter in $\{I_i\}_i$, $\{E_i\}_i$ and $\{T_i\}_i$.

```{r}
# bootstrap

```

## (f)

Use the following measure to compute the accuracy of the predicted secondary
structure $P = (p_i)$ for the `data.frame` of `proteins_test.tsv` given the real
secondary structure $S = (s_i)$:

$$
a(P, S) =
\frac{1}{L} \sum_{i} \left\{\begin{aligned}
&1 & \text{if } p_i = s_i\\
&0 & \text{if } p_i \neq s_i
\end{aligned}\right.
$$

with sequence length $L$. Compute the accuracy for every protein in your
`data.frame` and store the accuracies in a vector. What is the accuracy of the
Viterbi algorithm over all sequences (i.e. call `summary` on the vector of
accuracies)?

## (g)

Instead of using the Viterbi algorithm, now randomly guess secondary structures
for all sequences. Compare the global accuracies of the Viterbi and the random
approach and plot all accuracy distributions using boxplots.
