---
title: "Project 6 Sampling and variational inference"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author: "Team K - Minghang Li"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: [ "amsthm", "amssymb", "cancel", "kpfonts", "unicode-math" ]
---

I'm sorry that I didn't manage to finish the project on time :(
but I do want to get feedback for my finished version.

Please give points using my unfinished version -- or if you would be so kind,
give points using my finished version with late submission deduction for the
 parts I failed to finish on time.

# Problem 15: Monte Carlo estimation of an expected value

## Proof that $\mathbb{E}[\hat{g}(\mathbf{X})] = \mathbf{E}[g(X)]$

\begin{proof}

It's almost trivial that $\mathbb{E}[g(X)] = \mathbb{E}[g(X_i)]$ (because they
 are i.i.d from the same probability distribution as $X$).

$$
\begin{aligned}
\mathbb{E}[\hat{g}(\mathbf{X})] &=
\mathbb{E}\left[\frac{1}{N} \sum_{i=1}^N g(X_i)\right] \\
&=\frac{1}{N} \mathbb{E}\left[\sum_{i=1}^{N}g(X_i)\right] \\
&= \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}[g(X_i)] \\
&= \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}[g(X)] \\
&= \frac{N}{N} \mathbb{E}[g(X)] \\
&= \mathbb{E}[g(X)]
\end{aligned}
$$

\end{proof}


## Proof that $Var(\hat{g}(\mathbf{X})) = \frac{Var(g(X))}{N}$


\begin{proof}

By Bienayme's identity, we know that for pairwise independent variables, we have
 $Var\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^n Var(X_i)$.

And we also have $Var(g(X_i)) = Var(g(X))$ (by the same reason stated in the
 first subquestion, they are i.i.d with the same variance as $Var(g(X))$).

The proof then follows:

$$
\begin{aligned}
Var(\hat{g}(X))
&= Var\left(\frac{1}{N}\sum_{i=1}^N g(X_i)\right) \\
&= \frac{1}{N^2}Var\left(\sum_{i=1}^N g(X_i)\right) \\
&= \frac{1}{N^2} \sum_{i=1}^N Var(g(X_i)) \\
&= \frac{1}{N^2} \sum_{i=1}^N Var(g(X)) \\
&= \frac{N}{N^2} Var(g(X)) \\
&= \frac{Var(g(X))}{N}
\end{aligned}
$$

\end{proof}

<!-- Think about if those results also apply if X1, \..., XN are generated from a MCMC sampler (no need for demonstration here). -->

# Problem 16: Sampling in the Rain Network

## (a) Derive the expressions

All "$=T$"s are grayed out in the derivation for the ease of my brain to interpret.

Recall that given a Bayesian network with nodes in set $X$, for a certain node
 $x$ we have $P(x|X_{\setminus x}) = P(x|MB(x))$.

From the structure of Markove Chain we can know that $S \perp R | C$

With the knowledge learned at hand, we can derive the expressions:

\newcommand{\eqT}{{\textcolor{lightgray}{=T}}}
\newcommand{\eqF}{{\textcolor{black}{=F}}}

### (a). Derive $P(C_{\eqT}|R_\eqT, S_\eqT, W_\eqT)$

$$
\begin{aligned}
P(C_{\eqT} | R_\eqT,S_\eqT,W_\eqT)
&= P(C_\eqT | R_\eqT, S_\eqT)
& (\text{because }MB(C) = \{R, S\}) \\
&= \frac{P(R_\eqT, S_\eqT | C_\eqT)P(C_\eqT)}{P(R_\eqT, S_\eqT)} \\
&= \frac{P(R_\eqT | C_\eqT) P(S_\eqT | C_\eqT) P(C_\eqT)}{
  \sum_C P(R_\eqT | C) P(S_\eqT | C) P(C)
}
& (\text{because } S \perp R | C)
\end{aligned}
$$

From the Bayesian network we know that:

$$
\begin{aligned}
&P(C_\eqT) &&= 0.5\\
&P(R_\eqT|C_\eqT) &&=0.8\\
&P(S_\eqT|C_\eqT) &&=0.1\\
&P(R_\eqT, S_\eqT) &&=
P(R_\eqT|C_\eqT)P(S_\eqT|C_\eqT)P(C_\eqT)
+ P(R_\eqT|C_\eqF)P(S_\eqT|C_\eqF)P(C_\eqF) \\
&&&= 0.8\cdot0.1\cdot0.5 + 0.2\cdot0.5\cdot0.5\\
&&&= 0.09
\end{aligned}
$$

Plug in the values, $P(C_\eqT|R_\eqT,S_\eqT,W_\eqT)$ is hence calculated (by
 the following code blocks) to be $\frac{4}{9}$

```{r}
P_C_T <- 0.5
P_C_F <- 0.5
P_R_T_given_C_T <- 0.8
P_R_T_given_C_F <- 0.2
P_S_T_given_C_T <- 0.1
P_S_T_given_C_F <- 0.5

P_R_T_and_S_T <- P_R_T_given_C_T * P_S_T_given_C_T * P_C_T +
                 P_R_T_given_C_F * P_S_T_given_C_F * P_C_F
```

```{r}
P_C_T_give_R_T_and_S_T <- (P_R_T_given_C_T * P_S_T_given_C_T * P_C_T) / P_R_T_and_S_T
P_C_T_give_R_T_and_S_T
```

### 2. Derive $P(C_{\eqT}|R_\eqF, S_\eqT, W_\eqT)$

$$
\begin{aligned}
&P(C_\eqT | R_\eqF,S_\eqT,W_\eqT)
&&= P(C_\eqT | R_\eqF, S_\eqT)
& (\text{because }MB(C) = \{R, S\}) \\
&&&= \frac{P(R_\eqF, S_\eqT | C_\eqT)P(C_\eqT)}{P(R_\eqF, S_\eqT)} \\
&&&= \frac{P(R_\eqF | C_\eqT) P(S_\eqT | C_\eqT) P(C_\eqT)}{
  \sum_C P(R_\eqF | C) P(S_\eqT | C) P(C)
}
& (\text{because } S \perp R | C)
\end{aligned}
$$

We know that $S$ and $R$ are binary variables that only take `True` or `False`,
 i.e., $P(R_\eqT) + P(R_\eqF) = 1$ and $P(S_\eqT) + P(S_\eqF) = 1$.

Plug in the values, $P(C_\eqT|R_\eqF,S_\eqT,W_\eqT)$ is hence calculated (by the
 following code blocks) to be $\frac{1}{21}$

```{r}
P_R_F_given_C_T <- 1 - P_R_T_given_C_T
P_R_F_given_C_F <- 1 - P_R_F_given_C_T
P_R_F_and_S_T <- P_R_F_given_C_T * P_S_T_given_C_T * P_C_T +
                 P_R_F_given_C_F * P_S_T_given_C_F * P_C_F
```


```{r}
P_C_T_give_R_F_and_S_T <- (P_R_F_given_C_T * P_S_T_given_C_T * P_C_T) / P_R_F_and_S_T
P_C_T_give_R_F_and_S_T
```

### 3. Derive $P(R_{\eqT}|C_\eqT, S_\eqT, W_\eqT)$

In the following derivation the entities that are canceled out inside the
condition probability are either because $S \perp R | C$ or
$P(W|X_{\setminus{W}}) = P(W|MB(W))$.

$$
\begin{aligned}
P (R_\eqT | C_\eqT, S_\eqT, W_\eqT)
&= \frac{P(R_\eqT, C_\eqT, S_\eqT, W_\eqT)}{P(C_\eqT, S_\eqT, W_\eqT)} \\
&= \frac{
  P(W_\eqT | R_\eqT, C_\eqT, S_\eqT)
  P(R_\eqT, C_\eqT, S_\eqT)
}{
  P(W_\eqT | C_\eqT, S_\eqT) P(C_\eqT, S_\eqT)
} \\
&= \frac{
  P(W_\eqT | R_\eqT, \cancel{C_\eqT}, S_\eqT)
  P(R_\eqT | C_\eqT, \cancel{S_\eqT}) \cancel{P(C_\eqT, S_\eqT)}
}{
  P(W_\eqT | C_\eqT, S_\eqT) \cancel{P(C_\eqT, S_\eqT)}
} \\
&=\frac{
  P(W_\eqT | R_\eqT, S_\eqT)
  P(R_\eqT | C_\eqT)
}{
  \sum_R P(W_\eqT, R| C_\eqT, S_\eqT)
} \\
&= \frac{
  P(W_\eqT | R_\eqT, S_\eqT)
  P(R_\eqT | C_\eqT)
}{
  \sum_R P(W_\eqT| R, \cancel{C_\eqT}, S_\eqT) P(R| C_\eqT, \cancel{S_\eqT})
}& (\text{Trying to make use of } P(W|R, S)) \\
&= \frac{
  P(W_\eqT | R_\eqT, S_\eqT)
  P(R_\eqT | C_\eqT)
}{
  \sum_R P(W_\eqT| R, S_\eqT) P(R| C_\eqT)
}
\end{aligned}
$$
From the Bayesian network we know that

$$
\begin{aligned}
&P(W_\eqT | R_\eqT, S_\eqT) &&= 0.99 \\
&P(R_\eqT | C_\eqT)         &&= 0.8  \\
&\sum_R P(W_\eqT| R, S_\eqT) P(R| C_\eqT)
 &&= P(W_\eqT| R_\eqT, S_\eqT) P(R_\eqT| C_\eqT) +
     P(W_\eqT| R_\eqF, S_\eqT) P(R_\eqF| C_\eqT) \\
&&&= 0.99 \cdot 0.8 + 0.9 \cdot (1 - 0.8) \\
&&&= 0.972
\end{aligned}
$$
Plug in the values, we can find that $P(R_{\eqT}|C_\eqT, S_\eqT, W_\eqT) = \frac{22}{27}$

```{r}
P_W_T_given_R_T_and_S_T <- 0.99
P_W_T_given_R_F_and_S_T <- 0.9
P_W_T_given_R_T_and_S_F <- 0.9
P_W_T_given_R_F_and_S_F <- 0.1
```

```{r}
P_W_T_given_S_T_and_C_T <- P_W_T_given_R_T_and_S_T * P_R_T_given_C_T +
                           P_W_T_given_R_F_and_S_T * P_R_F_given_C_T
```

```{r}
P_R_T_given_C_T_and_S_T_and_W_T <- (P_W_T_given_R_T_and_S_T * P_R_T_given_C_T) /
                                   P_W_T_given_S_T_and_C_T
P_R_T_given_C_T_and_S_T_and_W_T
```

### 4. Derive $P(R_\eqT|C_\eqF, S_\eqT, W_\eqT)$

Following similar derivation as $P(R_\eqT|C_\eqT, S_\eqT, W_\eqT)$

$$
\begin{aligned}
P(R_\eqT|C_\eqF,S_\eqT,W_\eqT)\\
&= \frac{
  P(W_\eqT|R_\eqT,S_\eqT)P(R_\eqT|C_\eqF)
}{
  \sum_R P(W_\eqT|R,S_\eqT)P(R|C_\eqF)
} \\
&= \frac{0.99 \cdot 0.2}{
  0.99 \cdot 0.2 + 0.9 \cdot (1 - 0.2)} \\
&= \frac{11}{51}
\end{aligned}
$$

```{r}
P_R_T_given_C_F_and_S_T_and_W_T <- (P_W_T_given_R_T_and_S_T * P_R_T_given_C_F) /
                                  (P_W_T_given_R_T_and_S_T * P_R_T_given_C_F +
                                   P_W_T_given_R_F_and_S_T * P_R_F_given_C_F)
P_R_T_given_C_F_and_S_T_and_W_T
```

## (b). Implement the Gibbs sampler for the Bayesian network

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(coda)
library(doParallel)
library(doRNG)
library(foreach)
library(Rlab)
```


```{r}
sample_CgivenR <- function(R){
  C <- 0
  if (R == F) {
    C <- rbern(1, p=P_C_T_give_R_F_and_S_T)
  } else {
    C <- rbern(1, p=P_C_T_give_R_T_and_S_T)
  }
  return(C)
}

sample_RgivenC <- function(C){
  R <- 0
  if (C == F) {
    R <- rbern(1, p=P_R_T_given_C_F_and_S_T_and_W_T)
  } else {
    R <- rbern(1, p=P_R_T_given_C_T_and_S_T_and_W_T)
  }
  return(R)
}
```

```{r}
set.seed(2023)
```

```{r}
gibbs_sampler <- function(niter) {
  C <- rep(0,niter)
  R <- rep(0,niter)
  C[1]=1
  R[1]=1
  foreach (i=2:niter) %do% {
    C[i] <- sample_CgivenR(R[i-1])
    R[i] <- sample_RgivenC(C[i])
  }
  res <- data.frame(C=C,R=R)
  return(res)
}

```

```{r}
samples <- gibbs_sampler(100)
res <- table(samples) / 100
res
```

## (c). Estimate the marginal probability of rain ($P(R_\eqT | S_\eqT, W_\eqT)$)

Just sum over the $R=T$ column of the result table:

```{r}
P_R_T_given_S_T_and_W_T <- colSums(res)[[2]]
P_R_T_given_S_T_and_W_T
```

## (d). Use the R function `acf()` to provide plots and estimates of the auto-correlation functions for the samples of both variables Rain and Cloudy. Provide estimates of the ESS.


```{r}
acf_C <- acf(samples$C)
```
```{r}
ESS_C <- acf_C$n.used / (1 + 2 *sum(acf_C$acf))
ESS_C
```

```{r}
acf_R <- acf(samples$R)
```

```{r}
ESS_R <- acf_R$n.used / (1 + 2 *sum(acf_R$acf))
ESS_R
```

## (e) Draw 50,000 samples

```{r}
samples_50k_run1 <- gibbs_sampler(50000)
res_50k_run1 <- table(samples_50k_run1) / 50000
res_50k_run1
```

```{r}
samples_50k_run2 <- gibbs_sampler(50000)
res_50k_run2 <- table(samples_50k_run2) / 50000
res_50k_run2
```

## (f) Plot the relative frequenceis of $R=T$ and $C=T$ up to each iteration $t$ against $t$, for two independent runs of the sampler

```{r}
calc_relative_freq <- function(samples) {
  n <- length(x = samples)
  rel_freq <- foreach(t=1:n, .combine = c) %dopar% {
    return(sum(samples[1:t]/t))
  }
  return(rel_freq)
}
```

```{r}
num_cores <- detectCores()
registerDoParallel(num_cores)
start_time <- Sys.time()
rel_freq_C_run1 <- calc_relative_freq(samples = samples_50k_run1$C)
rel_freq_C_run2 <- calc_relative_freq(samples = samples_50k_run2$C)
rel_freq_R_run1 <- calc_relative_freq(samples = samples_50k_run1$R)
rel_freq_R_run2 <- calc_relative_freq(samples = samples_50k_run2$R)
end_time <- Sys.time()
end_time - start_time
stopImplicitCluster()
```
Plot the lines

```{r}
x=1:50000
plot(x, rel_freq_C_run1, col = "red",
     type="l",
     xlab = "Iteration #",
     ylab = "Relative Frequency",
)
lines(x, rel_freq_C_run2, col = "blue")

# Add the third vector, rel_freq_R_run1, to the plot
lines(x, rel_freq_R_run1, col = "green")

# Add the fourth vector, rel_freq_R_run2, to the plot
lines(x, rel_freq_R_run2, col = "orange")

# Add a legend to the plot
legend("topright",
       legend = c("C_run1", "C_run2", "R_run1", "R_run2"),
       col = c("red", "blue", "green", "orange"),
       lty = 1)

# Add vertical lines at x = 7000 and x = 15000
abline(v = c(7000, 15000), lty = 2, col = "black")
```

It seems that the relative frequencies start to plateau at around 7000 iterations.
Hence it's sufficient to do about 7000 iterations.

(And if you really want to be confident, maybe 15,000 iterations)

## (g) Apply the *Gelman and Rubin* test and plot the potential scale reduction factor changes over the iterations using `gelman.plot()`.

```{r}
mcmcs<- mcmc.list(mcmc(data = samples_50k_run1, start = 1, end = 5e4),
                  mcmc(data = samples_50k_run2, start = 1, end = 5e4))
gelman.diag(mcmcs)
gelman.plot(mcmcs)
```

It's clear that after about 7000 iterations there are almost no fluctuations in the
shrink factors and they go very close to 1.0. It would be safe to set a brun-in
time of about 7000, or to be safer, 10000.

## (h) Re-estimate $P(R=T|S=T,W=T)$ based on samples obtained after the burn-in time.

```{r}
samples_cut <- samples_50k_run1[1e4 + 1:5e4, ]
```

```{r}
res_cut <- table(samples_cut) / 4e4
res_cut
```
```{r}
P_R_T_given_S_T_and_W_T_reestimated <- colSums(res_cut)[[2]]
P_R_T_given_S_T_and_W_T_reestimated
```
## (i) Compute the probability analytically and compare with (c) and (h)

### Estimation from (c)

```{r, echo=FALSE}
cat("The value computed in (c) is: ", P_R_T_given_S_T_and_W_T)
```

### Estimation from (h)

```{r, echo=FALSE}
cat("The value computed in (h) is: ", P_R_T_given_S_T_and_W_T_reestimated)
```

## Analytical compuation

$$
\begin{aligned}
P(R_\eqT | S_\eqT, W_\eqT)
&= \frac{P(R_\eqT, S_\eqT, W_\eqT)}{P(S_\eqT, W_\eqT)} \\
&= \frac{\sum_C P(R_\eqT, S_\eqT, W_\eqT, C)}{\sum_C P(S_\eqT, W_\eqT, C)}
\end{aligned}
$$

The numerator part:

$$
\begin{aligned}
\sum_C P(R_\eqT, S_\eqT, W_\eqT, C)
&= \sum_C P(R_\eqT, S_\eqT, W_\eqT | C)P(C) \\
&= \sum_C P(W_\eqT | R_\eqT, S_\eqT, \cancel{C})P(R_\eqT, S_\eqT|C)P(C) \\
&= \sum_C P(W_\eqT | R_\eqT, S_\eqT)P(R_\eqT|C)P(S_\eqT|C)P(C) \\
&= P(W_\eqT | R_\eqT, S_\eqT)P(R_\eqT|C_\eqT)P(S_\eqT|C_\eqT)P(C_\eqT) \\
&\phantom{=}+ P(W_\eqT | R_\eqT, S_\eqT)P(R_\eqT|C_\eqF)P(S_\eqT|C_\eqF)P(C_\eqF) \\
&= 0.99 \cdot 0.8 \cdot 0.1 \cdot 0.5 + 0.99 \cdot 0.2 \cdot 0.5 \cdot 0.5\\
&= 0.0891
\end{aligned}
$$
```{r}
numerator <- P_W_T_given_R_T_and_S_T * P_R_T_given_C_T * P_S_T_given_C_T * P_C_T +
  P_W_T_given_R_T_and_S_T * P_R_T_given_C_F * P_S_T_given_C_F * P_C_F
numerator
```
The denominator:

$$
\begin{aligned}
\sum_C P(S_\eqT, W_\eqT, C)
&= \sum_C \sum_R P(W_\eqT, S_\eqT, R, C) \\
&= \sum_C \sum_R P(W_\eqT | R, S_\eqT, \cancel{C})P(R, S_\eqT, C) \\
&= \sum_C \sum_R P(W_\eqT | R, S_\eqT) P(R|C) P(S_\eqT| C)P(C) \\
&= P(W_\eqT | R_\eqT, S_\eqT) P(R_\eqT|C_\eqT) P(S_\eqT| C_\eqT)P(C_\eqT) \\
&\phantom{=}
+ P(W_\eqT | R_\eqF, S_\eqT) P(R_\eqF|C_\eqT) P(S_\eqT| C_\eqT)P(C_\eqT) \\
&\phantom{=}
+ P(W_\eqT | R_\eqT, S_\eqT) P(R_\eqT|C_\eqF) P(S_\eqT| C_\eqF)P(C_\eqF) \\
&\phantom{=}
+ P(W_\eqT | R_\eqF, S_\eqT) P(R_\eqF|C_\eqF) P(S_\eqT| C_\eqF)P(C_\eqF) \\
&= 0.99 \cdot 0.8 \cdot 0.1 \cdot 0.5 +
   0.9  \cdot (1 - 0.8) \cdot 0.1 \cdot 0.5  \\
&\phantom{=}+ 0.99 \cdot 0.2 \cdot 0.5 \cdot 0.5 +
   0.9  \cdot (1-0.2) \cdot 0.5 \cdot 0.5 \\
&= 0.2781
\end{aligned}
$$
```{r}
denominator <- P_W_T_given_R_T_and_S_T * P_R_T_given_C_T * P_S_T_given_C_T * P_C_T +
  P_W_T_given_R_F_and_S_T * P_R_F_given_C_T * P_S_T_given_C_T * P_C_T +
  P_W_T_given_R_T_and_S_T * P_R_T_given_C_F * P_S_T_given_C_F * P_C_F +
  P_W_T_given_R_F_and_S_T * P_R_F_given_C_F * P_S_T_given_C_F * P_C_F
denominator
```

Plug back the values of the denominator and numerator, we have:

$$
P(R_\eqT | S_\eqT, W_\eqT) = \frac{0.0891}{0.2781} \approx 0.3204
$$

```{r}
P_R_T_given_S_T_and_W_T_analytic <- numerator / denominator
P_R_T_given_S_T_and_W_T_analytic
```
\pagebreak

```{r}
# Create a data frame with the three probabilities
probs <- data.frame(
  Probability = c("Estimation in (c)", "Re-estimation in (h)", "Analytic solution"),
  Value = c(P_R_T_given_S_T_and_W_T,
            P_R_T_given_S_T_and_W_T_reestimated,
            P_R_T_given_S_T_and_W_T_analytic)
)

# Use knitr::kable() to format the data frame as a table
knitr::kable(probs, align = "l", caption = "Probabilities", row.names = FALSE)
```

The value obtained in (h) seems to be very close to the analytical solution.
The value obtained in (c) is more different from the analytical solution but is
 expected since we only sampled 100 times.
