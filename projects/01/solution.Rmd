---
title: "Project 1 Bayesian Networks and Gene Regulation"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
author: "Team C - Minghang Li, Xiaocheng Yang, Xinyi Chen"
mainfont: "KpRoman"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:  [ "amsmath", "amsthm", "amssymb", "cancel", "unicode-math" ]
---

```{r install_from_BiocManager, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(c("graph", "Rgraphviz", "RBGL"),
                     update = FALSE)
```

```{r install_packages, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
list.of.packages <- c("foreach", "doParallel", "doRNG")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```

```{r load_packages, echo=FALSE, message=FALSE, warning=FALSE}
library(foreach)
library(doParallel)
library(doRNG)
library(GGally)
library(BiDAG)
library(igraph)
library(graph)
```


# Problem 1: Conditional independence and BNs

## (a)

$A \perp B | C$ holds, but $A \perp B$ does not hold.

### Proof for $A \perp B | C$

From the BN, we know that

$$
P(A,B,C) = P(A|C) P(B|C) P(C)
$$

To prove that $A \perp B | C$, we need to show that $P(A, B|C) = P(A|C)P(B|C)$.

$$
\begin{aligned}
P(A,B|C) &= \frac{P(A, B, C)}{P(C)} \\
&= \frac{P(A|C)P(B|C)P(C)}{P(C)} \\
&= P(A|C)P(B|C)
\end{aligned}
$$

### Disproof for $A \perp B$

Consider $P(A, B) = \sum_{C} P(A, B, C)$. From the BN, we have:

$$
\begin{aligned}
P(A, B) &= \sum_{C} P(A,B,C) \\
        &= \sum_{C} P(A|C)P(B|C)P(C) \\
        &= \sum_{C} \frac{P(C|A)P(A)}{P(C)}\frac{P(C|B)P(B)}{P(C)}P(C) \\
        &= P(A)P(B) \sum_{C} \frac{P(C|A)P(C|B)}{P(C)} \\
        &\neq P(A)P(B) & \text{(usually)}
\end{aligned}
$$

Hence, in general $A \perp B$ does not hold for this Bayesian network.

## (b)

$A \perp B | C$ does not hold, but $A \perp B$ holds

### Disproof for $A \perp B | C$

Consider $A \perp B | C$, we'll have $P(A, B|C) = P(A|C)P(B|C)$.

From the BN and by applying Bayes theorem we can derive that:

$$
\begin{aligned}
P(A, B|C) &= \frac{P(A, B, C)}{P(C)} \\
          &= \frac{P(A)P(B)P(C|A,B)}{P(C)} \\
          &= \frac{P(A)P(B)P(C|A,B)}{P(C)} \frac{P(C|A)}{P(C|A)}\frac{P(C|B)}{P(C|B)} \\
          &= \frac{P(C|A)P(A)}{P(C)}\frac{P(C|B)P(B)}{P(C)}\frac{P(C)P(C|A,B)}{P(C|A)P(C|B)} \\
          &= P(A|C)P(B|C)\frac{P(C)P(C|A,B)}{P(C|A)P(C|B)} \\
          &\neq P(A|C)P(B|C) & \text{(usually)}
\end{aligned}
$$

### Proof for $A \perp B$

From the BN, we know that

$$
P(A,B,C) = P(A)P(B)P(C|A,B)
$$

We know from Bayes theorem that $P(A, B, C) = P(C|A,B)P(A,B)$.

Hence we have:

$$
P(C|A,B)P(A)P(B) = P(C|A,B)P(A,B)
$$

i.e., $P(A, B) = P(A)P(B)$. We have consequently proven that $A \perp B$.

# Problem 2: Markov blanket

The Markov blanket $MB(D)$ is $\{B, C, E, F, G\}$, where $B$ and $F$ are the
parents of $D$, $C$ and $G$ are the children of $D$, and $E$ is the co-parent
of $D$.

To prove that the conditional probability of $P(X_k|X_{n\neq k})$ is equivalent
to $P(X_k|MB(X_k))$, we can prove that $\forall X_{j}$ where
$j \in [1, n], j\neq k$, if $X_j \notin MB(X_k)$, then $X_j \perp X_k | MB(X_k)$.

For this specific question, we need to prove that $A \perp D | MB(D)$. It is
obvious that $A \perp D | MB(D)$ because $A$ and $D$ are d-separated given
$MB(D)$ since $E \in MB(D)$ is on the (only) path from $A$ to $D$ and $E$ is in
a cascade structure $A \rightarrow E \rightarrow G$.

Alternatively it can also be proved by maticulously simplifying the total probability (since d-seperation is not introduced yet):

$$
\begin{aligned}
P(D|A,B,C,E,F,G) &= \frac{P(A,B,C,D,E,F,G)}{P(A,B,C,E,F,G)} \\
&= \frac{
  \cancel{P(A)}\cancel{P(B)}\cancel{P(F)}P(D|B,F)\cancel{P(E|A)}P(G|D,E)P(C|D)
}{
  \sum_{D}\cancel{P(A)}\cancel{P(B)}\cancel{P(F)}P(D|B,F)\cancel{P(E|A)}P(G|D,E)P(C|D)
} \\
&= \frac{P(D|B,F)P(G|D,E)P(C|D)}{\sum_{D}P(D|B,F)P(G|D,E)P(C|D)}
\end{aligned}
$$

It's clear that the total probability depends only on $MB(D)$. Hence we have $P(D|A,B,C,E,F,G) = P(D|MB(D))$

# Problem 3: Learning Bayesian networks from protein data

## Package and Dataset Preparation

```{r set_random_seed, tidy=TRUE, tidy.opts=list(width.cutoff=80)}
set.seed(42)
```

```{r get_data}
# download dataset
df <- read.csv(file="https://raw.githubusercontent.com/felixleopoldo/benchpress/master/resources/data/mydatasets/2005_sachs_2_cd3cd28icam2_log_std.csv")
head(df)
```

## (a)

Number of variables ($n$): $11$ Number of observations ($N$): $902$

```{r show_df_dim}
dim(df)
```

Visualization of the transformed data using `ggpairs` function.

```{r visualize_data, results='hide', fig.keep='all', message=FALSE}
ggpairs(df, diag = list(continuous = "barDiag"))
```

Randomly split the data into 80% traning data and 20% test data.

```{r split_train_test_data}
train_data_size <- floor(0.8*nrow(df))
picked <- sample(seq_len(nrow(df)), size=train_data_size)
train_data <- df[picked, ]
test_data  <- df[-picked,]
```

Initialize the parameters using the function `BiDAG::scoreparameters` with the
training data and the Bayesian Gaussian equivalent (BGe) score.

```{r get_score}
train_scorepar <- BiDAG::scoreparameters(scoretype="bge", train_data)
```

## (b)

Learn a Bayesian network using the `BiDAG::iterativeMCMC` function.

```{r iterativeMCMC}
BN <- iterativeMCMC(scorepar=train_scorepar, verbose=FALSE)
```

```{r summary_BN}
summary(BN)
```

Plot the DAG.

```{r plot_DAG}
par(mfrow=c(1, 1))
plot(graphAM(as.matrix(BN$DAG), edgemode = "directed"))
```

Evaluate the log score of the test data against the estimated DAG using
`BiDAG::scoreagainstDAG`.

```{r get_logscore}
sum(scoreagainstDAG(scorepar = train_scorepar, BN$DAG, test_data))
```

## (c)

The following code blocks run 10 iterations for each experiments.

```{r create_result_df}
res <- data.frame(matrix(vector(mode = 'numeric',length = 10), ncol=5, nrow=2))
colnames(res) <- c(1, 2, 3, 4, 5)
rownames(res) <- c("ecount", "logscore")
```

```{r define_am_list}
am_list <- 10^(c(-5, -3, -1, 1, 2))
```

```{r run_am_procedure, results='hide'}
num_cores <- detectCores()
registerDoParallel(num_cores)
index <- 1
foreach (am=am_list) %do% {
    # by default doRNG uses "Lâ€™Ecuyer-CMRG" so should be fine
    v <- foreach (i=1:10, .combine=rbind) %dorng% {
        picked <- sample(seq_len(nrow(df)), size=train_data_size)
        train_data <- df[picked, ]
        test_data  <- df[-picked,]
        train_scorepar <- BiDAG::scoreparameters(
            scoretype="bge", train_data,
            bgepar=list(am=am, aw=NULL)
        )
        BN <- BiDAG::iterativeMCMC(scorepar=train_scorepar)
        ecount <- sum(BN$DAG)
        log_score <- sum(BiDAG::scoreagainstDAG(train_scorepar, BN$DAG, test_data))
        return(c(ecount=ecount, log_score=log_score))
    }
    avg <- colMeans(v)
    res[1, index] <- avg[1]
    res[2, index] <- avg[2]
    index <- index + 1
}
stopImplicitCluster()
```

```{r print_results_dataframe}
colnames(res) <- am_list
print(res)
```

It seems that as `am` increase, the average number of edges also increases. 

The log score seems to have an optimum at `am=1` for the parameters we tested.

The final graph from the whole dataset is visualized in the following code block.

```{r plot_BN_from_whole_dataset}
scorepar <- BiDAG::scoreparameters(scoretype="bge", df, bgepar=list(am=1, aw=NULL))
BN <- BiDAG::iterativeMCMC(scorepar=scorepar, verbose=FALSE)
par(mfrow=c(1,1))
plot(graphAM(as.matrix(BN$DAG), edgemode = "directed"))
```
